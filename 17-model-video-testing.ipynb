{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d7ada-2fb1-4ccf-90a5-1a15da791b0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88c6179-2db2-48f0-a8a2-306baa2eaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ac90e6-5985-48ab-bebc-c39ee21dadb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All loaded. Ready for prediction.\n"
     ]
    }
   ],
   "source": [
    "# MODEL VIDEO TESTING\n",
    "# Base Parameters\n",
    "WINDOW = 30  # frames\n",
    "STRIDE = 10   # next window step\n",
    "K_CONSEC_FALL = 8  # consecutive frames to alert\n",
    "K_CONSEC_ATT = 5\n",
    "CONFIG = \"lstm\"\n",
    "CONFIG_LABEL = f\"{WINDOW}_s{STRIDE}_kf{K_CONSEC_FALL}_ka{K_CONSEC_ATT}\"\n",
    "# Parameters\n",
    "NUM_CLASSES = 3\n",
    "OUTPUT_BASE_PATH = \"results/fall_detect_yolo11n_pose_balanced\"\n",
    "OUTPUT_DATASET_DIR = f\"{OUTPUT_BASE_PATH}/windows_{CONFIG_LABEL}/\"\n",
    "OUTPUT_LSTM_MODEL_DIR = f\"{OUTPUT_BASE_PATH}/lstm_model_w{CONFIG_LABEL}_{CONFIG}\"\n",
    "OUTPUT_LSTM_MODEL_FULL = \"lstm_model_full.keras\"\n",
    "OUTPUT_LSTM_MODEL_BEST = \"lstm_model_best.keras\"\n",
    "OUTPUT_LSTM_MODEL_HISTORY = \"lstm_model_history.json\"\n",
    "VIDEOS_TEST_PATH = \"videos_test\"\n",
    "SCALER_PATH = f\"{OUTPUT_LSTM_MODEL_DIR}/train_test_split/scaler.joblib\"\n",
    "YOLO_MODEL_PATH = f\"{OUTPUT_BASE_PATH}/yolo11n_pose_train/weights/best.pt\"\n",
    "LSTM_MODEL_PATH = f\"{OUTPUT_LSTM_MODEL_DIR}/{OUTPUT_LSTM_MODEL_BEST}\"\n",
    "MODEL_TEST_PATH = f\"{OUTPUT_BASE_PATH}/model_test_w{CONFIG_LABEL}_{CONFIG}\"\n",
    "os.makedirs(MODEL_TEST_PATH, exist_ok=True)\n",
    "LABELS = {0: \"no_fall\", 1: \"fall\", 2: \"attention\"}\n",
    "#LABELS = ['no_fall', 'fall', 'attention']\n",
    "\n",
    "# Check if model files exist\n",
    "if not os.path.exists(YOLO_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"YOLO model not found in {YOLO_MODEL_PATH}\")\n",
    "if not os.path.exists(LSTM_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"LSTM model not found in {LSTM_MODEL_PATH}\")\n",
    "    \n",
    "model_yolo = YOLO(YOLO_MODEL_PATH)\n",
    "model_lstm = load_model(LSTM_MODEL_PATH)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "print(\"All loaded. Ready for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec498ecf-40f5-452b-9d1f-61953361b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script model testing\n",
    "def extract_box_features(box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    x_center = x1 + width / 2\n",
    "    y_center = y1 + height / 2\n",
    "    area = width * height\n",
    "    aspect_ratio = width / height if height != 0 else 0\n",
    "    return np.array([x1, y1, x2, y2, x_center, y_center, width, height, area, aspect_ratio], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e6f9ad9-beed-4431-9a3f-c61994b3d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_box_features(features, frame_shape):\n",
    "    frame_height, frame_width = frame_shape[:2]\n",
    "    max_area = frame_width * frame_height\n",
    "\n",
    "    # Normalize os valores com base na dimensão do frame\n",
    "    normalized = np.array([\n",
    "        features[0] / frame_width,     # x1\n",
    "        features[1] / frame_height,    # y1\n",
    "        features[2] / frame_width,     # x2\n",
    "        features[3] / frame_height,    # y2\n",
    "        features[4] / frame_width,     # x_center\n",
    "        features[5] / frame_height,    # y_center\n",
    "        features[6] / frame_width,     # width\n",
    "        features[7] / frame_height,    # height\n",
    "        features[8] / max_area,        # area\n",
    "        features[9]                    # aspect_ratio (já é uma razão)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7cef40-122c-4fa2-9df0-0a4127520c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_features(pose_data, frame_shape):\n",
    "    keypoints = []\n",
    "    frame_height, frame_width = frame_shape[:2]\n",
    "    \n",
    "    if pose_data is None or len(pose_data) == 0:\n",
    "        return np.zeros(54, dtype=np.float32)\n",
    "    \n",
    "    for x, y, c in pose_data:\n",
    "        #if c < 0.5: # ignora quando confiança é menor que 50% - evita ruídos\n",
    "        #    continue\n",
    "        keypoints.extend([x / frame_width, y / frame_height, c])\n",
    "    return np.array(keypoints, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa140195-cf2f-4618-8ce4-98cff416a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_features(curr: np.ndarray, prev: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Difference (frame_t - frame_{t-1}) in all normalized features.\"\"\"\n",
    "    if prev is None or prev.shape != curr.shape:\n",
    "        return np.zeros_like(curr, dtype=np.float32)\n",
    "    return (curr - prev).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c448980e-5f72-4c93-bfd0-f03ded11d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_video(video_path):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: video '{video_path}' not found.\")\n",
    "        return\n",
    "        \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening the video '{video_path}'\")\n",
    "        return\n",
    "\n",
    "    # Parâmetros para salvar o vídeo\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # ou 'XVID'\n",
    "    output = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total de frames: {total_frames}\")\n",
    "    \n",
    "    # Definir checkpoints de 10%\n",
    "    checkpoints = [int(total_frames * i / 10) for i in range(1, 11)]\n",
    "    # sequence = []\n",
    "    frame_count = 0\n",
    "    sequences_by_id = {}\n",
    "    prev_features_by_id = {}\n",
    "    predictions_by_id = {}\n",
    "    #all_sequences = []\n",
    "    #tick = 0\n",
    "\n",
    "    print(\"Processing video...\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        valid, frame = cap.read()\n",
    "        \n",
    "        if not valid:\n",
    "            print(\"End of video or error reading frame\")\n",
    "            break\n",
    "\n",
    "        # Check if frame is not empty\n",
    "        if frame is None or frame.size == 0:\n",
    "            print(\"Empty frame found\")\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        if frame_count in checkpoints:\n",
    "            percent = (frame_count / total_frames) * 100\n",
    "            print(f\"Processed {percent:.0f}% ({frame_count}/{total_frames})\")\n",
    "        \n",
    "        # YOLO: person detection\n",
    "        results = model_yolo.track(frame, verbose=False)\n",
    "        \n",
    "        #if len(results) == 0 or results[0].boxes is None or len(results[0].boxes) == 0:\n",
    "        #    print(f\"Frame {frame_count}: No detection\")\n",
    "            #cv2.imshow('Detecção de Queda', annotated_frame)\n",
    "            \n",
    "        #    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #        break\n",
    "        #    continue\n",
    "\n",
    "        if results[0].boxes is None or results[0].boxes.id is None:\n",
    "            output.write(annotated_frame)\n",
    "            continue\n",
    "\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        keypoints = results[0].keypoints.data.cpu().numpy()\n",
    "        track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "        #if len(boxes) == 0:\n",
    "        #    continue\n",
    "\n",
    "        for i, (box, kp, track_id) in enumerate(zip(boxes, keypoints, track_ids)):\n",
    "            label = classes[i]\n",
    "            #if label not in [0, 1, 2]:\n",
    "            #    continue\n",
    "\n",
    "            if track_id not in sequences_by_id:\n",
    "                sequences_by_id[track_id] = []\n",
    "                prev_features_by_id[track_id] = None\n",
    "                predictions_by_id[track_id] = (0, 0.0) # (class_id, confidence)\n",
    "\n",
    "            box_feat = extract_box_features(box)\n",
    "            norm_box = normalize_box_features(box_feat, frame.shape)\n",
    "            pose_feat = extract_pose_features(kp, frame.shape)\n",
    "\n",
    "            combined_static_features = np.concatenate([norm_box, pose_feat])\n",
    "            velocity_features = diff_features(combined_static_features, prev_features_by_id[track_id])\n",
    "            prev_features_by_id[track_id] = combined_static_features.copy()\n",
    "            all_features = np.concatenate([combined_static_features, velocity_features])\n",
    "            sequences_by_id[track_id].append((all_features)) #label\n",
    "\n",
    "            if len(sequences_by_id[track_id]) > WINDOW:\n",
    "                sequences_by_id[track_id].pop(0)\n",
    "                \n",
    "            #combined_feat = np.concatenate([norm_box, pose_feat])  # 64 features\n",
    "            #sequence.append(combined_feat)\n",
    "\n",
    "            if len(sequences_by_id[track_id]) == WINDOW:\n",
    "                #input_seq = np.expand_dims(np.array(current_sequence), axis=0)\n",
    "                input_seq = np.array(sequences_by_id[track_id])\n",
    "                input_scaled = scaler.transform(input_seq)\n",
    "                input_final = np.expand_dims(input_scaled, axis=0)\n",
    "\n",
    "                pred = model_lstm.predict(input_final, verbose=0)\n",
    "                class_id = np.argmax(pred)\n",
    "                confidence = pred[0][class_id]\n",
    "                #label_str = LABELS[class_id]\n",
    "                predictions_by_id[track_id] = (class_id, confidence) # store last prediction\n",
    "\n",
    "                for track_id in predictions_by_id:\n",
    "                    # Encontrar o box atual do track_id para desenhar\n",
    "                    current_box = None\n",
    "                    for i, tid in enumerate(track_ids):\n",
    "                        if tid == track_id:\n",
    "                            current_box = boxes[i]\n",
    "                            break\n",
    "                    \n",
    "                    if current_box is not None:\n",
    "                        class_id, confidence = predictions_by_id[track_id]\n",
    "                        label_str = LABELS.get(class_id, \"unknown\")\n",
    "                        color = {\n",
    "                            'no_fall': (0, 255, 0), \n",
    "                            'attention': (0, 255, 255), \n",
    "                            'fall': (0, 0, 255)\n",
    "                        }.get(label_str, (255, 255, 255))\n",
    "                        \n",
    "                        x1, y1, x2, y2 = current_box.astype(int)\n",
    "                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        cv2.putText(annotated_frame, f'ID {track_id}: {label_str} ({confidence:.2f})', (x1, y1 - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                        \n",
    "                # Draw prediction\n",
    "                #x1, y1, x2, y2 = box.astype(int)\n",
    "                # Class colors\n",
    "                #if label_str == 'no_fall':\n",
    "                #    color = (0, 255, 0)  # Verde\n",
    "                #elif label_str == 'attention':\n",
    "                #    color = (0, 255, 255)  # Amarelo\n",
    "                #elif label_str == 'fall':\n",
    "                #    color = (0, 0, 255)  # Vermelho\n",
    "                #else:\n",
    "                #    color = (255, 255, 255)  # Branco (fallback)\n",
    "\n",
    "                # Draw border and text\n",
    "                #cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                #cv2.putText(frame, f'{label_str} ({confidence:.2f})', (x1, y1 - 10),\n",
    "                #            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "                #print(f\"Predição frame: {frame_count}: {label} - Confiança: {confidence:.2f}\")\n",
    "                #sequence.pop(0)\n",
    "\n",
    "        output.write(annotated_frame)\n",
    "\n",
    "    cap.release()\n",
    "    output.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Video processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4795d75a-4862-4017-8005-337f4fcfaa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de frames: 188\n",
      "Processing video...\n",
      "Processed 10% (18/188)\n",
      "Processed 20% (37/188)\n",
      "Processed 30% (56/188)\n",
      "Processed 40% (75/188)\n",
      "Processed 50% (94/188)\n",
      "Processed 60% (112/188)\n",
      "Processed 70% (131/188)\n",
      "Processed 80% (150/188)\n",
      "Processed 90% (169/188)\n",
      "Processed 100% (188/188)\n",
      "End of video or error reading frame\n",
      "Video processing completed.\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = f\"{VIDEOS_TEST_PATH}/070.mp4\"\n",
    "OUTPUT_PATH = f\"{MODEL_TEST_PATH}/070A_w{CONFIG_LABEL}_{CONFIG}.mp4\"\n",
    "\n",
    "predict_from_video(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710fd2f-a086-48bf-a4c9-db369e0c2f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
