{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d7ada-2fb1-4ccf-90a5-1a15da791b0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88c6179-2db2-48f0-a8a2-306baa2eaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8195dae6-b17b-4665-90de-4b9310c871d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script LSTM Training\n",
    "# Path to the yolo trained model\n",
    "BASE_PATH = \"results/fall_detect_yolo11n_pose_balanced\"\n",
    "MODEL_PATH = f\"{BASE_PATH}/yolo11n_pose_train/weights/best.pt\"\n",
    "WINDOW = 30  # frames\n",
    "STRIDE = 10   # next window step\n",
    "K_CONSEC_FALL = 8  # consecutive frames to alert\n",
    "K_CONSEC_ATT = 5\n",
    "CONFIG_LABEL = f\"{WINDOW}_s{STRIDE}_kf{K_CONSEC_FALL}_ka{K_CONSEC_ATT}\"\n",
    "OUTPUT_DIR = f\"{BASE_PATH}/windows_{CONFIG_LABEL}/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loading of trained model\n",
    "model = YOLO(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec498ecf-40f5-452b-9d1f-61953361b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_box_features(box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    x_center = x1 + width / 2\n",
    "    y_center = y1 + height / 2\n",
    "    area = width * height\n",
    "    aspect_ratio = width / height if height != 0 else 0\n",
    "    return np.array([x1, y1, x2, y2, x_center, y_center, width, height, area, aspect_ratio], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6f9ad9-beed-4431-9a3f-c61994b3d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_box_features(features, frame_shape):\n",
    "    frame_height, frame_width = frame_shape[:2]\n",
    "    max_area = frame_width * frame_height\n",
    "\n",
    "    # Normalize os valores com base na dimensão do frame\n",
    "    normalized = np.array([\n",
    "        features[0] / frame_width,     # x1\n",
    "        features[1] / frame_height,    # y1\n",
    "        features[2] / frame_width,     # x2\n",
    "        features[3] / frame_height,    # y2\n",
    "        features[4] / frame_width,     # x_center\n",
    "        features[5] / frame_height,    # y_center\n",
    "        features[6] / frame_width,     # width\n",
    "        features[7] / frame_height,    # height\n",
    "        features[8] / max_area,        # area\n",
    "        features[9]                    # aspect_ratio (já é uma razão)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa7cef40-122c-4fa2-9df0-0a4127520c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_features(pose_data, frame_shape):\n",
    "    keypoints = []\n",
    "    frame_height, frame_width = frame_shape[:2]\n",
    "    \n",
    "    if pose_data is None or len(pose_data) == 0:\n",
    "        return np.zeros(54, dtype=np.float32)\n",
    "    \n",
    "    for x, y, c in pose_data:\n",
    "        #if c < 0.5: # ignora quando confiança é menor que 50% - evita ruídos\n",
    "        #    continue\n",
    "        keypoints.extend([x / frame_width, y / frame_height, c])\n",
    "    return np.array(keypoints, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3642ed-249e-47b6-8423-6ac91c57ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_features(curr: np.ndarray, prev: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Difference (frame_t - frame_{t-1}) in all normalized features.\"\"\"\n",
    "    if prev is None or prev.shape != curr.shape:\n",
    "        return np.zeros_like(curr, dtype=np.float32)\n",
    "    return (curr - prev).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e75b9e-d729-406c-8e35-c35a260967ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_k_consecutive(seq, target, k):\n",
    "    run = 0\n",
    "    for v in seq:\n",
    "        if v == target:\n",
    "            run += 1\n",
    "            if run >= k:\n",
    "                return True\n",
    "        else:\n",
    "            run = 0\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c778cd72-3ae0-4dfe-b466-67510021aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    FILE_DIR = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    output_path = os.path.join(OUTPUT_DIR, FILE_DIR)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    # sequence = []\n",
    "    # prev_features = None\n",
    "    sequences_by_id = {}\n",
    "    prev_features_by_id = {}\n",
    "    all_sequences = []\n",
    "    tick = 0\n",
    "\n",
    "    while True:\n",
    "        valid, frame = cap.read()\n",
    "        if not valid:\n",
    "            break\n",
    "\n",
    "        results = model.track(frame, verbose=False) # track one person - id\n",
    "        \n",
    "        #if len(results) == 0 or results[0].boxes is None or results[0].keypoints is None:\n",
    "        #    continue\n",
    "\n",
    "        if results[0].boxes is None or results[0].boxes.id is None:\n",
    "            continue\n",
    "\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        keypoints = results[0].keypoints.data.cpu().numpy()\n",
    "        track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "        for i, (box, kp, track_id) in enumerate(zip(boxes, keypoints, track_ids)):\n",
    "            label = classes[i]\n",
    "            if label not in [0, 1, 2]:\n",
    "                continue\n",
    "\n",
    "            if track_id not in sequences_by_id:\n",
    "                sequences_by_id[track_id] = []\n",
    "                prev_features_by_id[track_id] = None\n",
    "\n",
    "            current_sequence = sequences_by_id[track_id]\n",
    "            prev_features = prev_features_by_id[track_id]\n",
    "            \n",
    "            box_features = extract_box_features(box)\n",
    "            norm_box = normalize_box_features(box_features, frame.shape)\n",
    "            pose_kp = extract_pose_features(kp, frame.shape)\n",
    "            \n",
    "            combined_static_features = np.concatenate([norm_box, pose_kp])\n",
    "            velocity_features = diff_features(combined_static_features, prev_features)\n",
    "            \n",
    "            prev_features_by_id[track_id] = combined_static_features.copy()\n",
    "            all_features = np.concatenate([combined_static_features, velocity_features])\n",
    "            current_sequence.append((all_features, label))\n",
    "\n",
    "            if len(current_sequence) > WINDOW:\n",
    "                current_sequence.pop(0)\n",
    "    \n",
    "            if len(current_sequence) == WINDOW:\n",
    "                features = [ft for ft, _ in current_sequence]\n",
    "                labels = [lb for _, lb in current_sequence]\n",
    "\n",
    "                if has_k_consecutive(labels, 1, K_CONSEC_FALL):\n",
    "                    final_label = 1\n",
    "                elif has_k_consecutive(labels, 2, K_CONSEC_ATT):\n",
    "                    final_label = 2\n",
    "                else:\n",
    "                    final_label = max(set(labels), key=labels.count)  # fallback: most common\n",
    "\n",
    "                if tick % STRIDE == 0:\n",
    "                    np.save(os.path.join(\n",
    "                        output_path,\n",
    "                        f\"{FILE_DIR}_{len(all_sequences):03d}_{final_label}.npy\"\n",
    "                    ), np.array(features))\n",
    "                \n",
    "                all_sequences.append((np.array(features), final_label))\n",
    "                tick += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1a4cafb-fc1c-45f5-a376-0b2fdf3016c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos...\n",
      "Processed 10% (48/480)\n",
      "Processed 20% (96/480)\n",
      "Processed 30% (144/480)\n",
      "Processed 40% (192/480)\n",
      "Processed 50% (240/480)\n",
      "Processed 60% (288/480)\n",
      "Processed 70% (336/480)\n",
      "Processed 80% (384/480)\n",
      "Processed 90% (432/480)\n",
      "Processed 100% (480/480)\n",
      "Videos processed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Windows creation\n",
    "videos_dir = 'videos_aug/'\n",
    "print(f\"Processing videos...\")\n",
    "video_count = 0\n",
    "\n",
    "files = os.listdir(videos_dir)\n",
    "total_videos = len([f for f in files if os.path.isfile(os.path.join(videos_dir, f))])\n",
    "checkpoints = [int(total_videos * i / 10) for i in range(1, 11)]\n",
    "\n",
    "for i, file in enumerate(sorted(os.listdir(videos_dir))):\n",
    "    if file.endswith('.mp4'):\n",
    "        video_count += 1\n",
    "        \n",
    "        if video_count in checkpoints:\n",
    "            percent = (video_count / total_videos) * 100\n",
    "            print(f\"Processed {percent:.0f}% ({video_count}/{total_videos})\")\n",
    "            \n",
    "        # print(f\"Processando video {i}...\")\n",
    "        video_path = os.path.join(videos_dir, file)\n",
    "        process_video(video_path)\n",
    "\n",
    "print(\"Videos processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bb6b98b-0546-46b5-80a9-fafefe2f04ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.59319     0.36539     0.70941     0.93856      0.6513     0.65197     0.11623     0.57317    0.066619      0.3605     0.63194      0.4019     0.96163     0.63932     0.38244      0.9594     0.62282     0.38468     0.96503     0.65428     0.37357     0.96289     0.61346     0.37915     0.96498      0.6369\n",
      "      0.45787     0.97374     0.68717     0.46212     0.96718      0.5921     0.47803     0.96541     0.68985     0.61662     0.96707     0.60333      0.6239     0.96226     0.66385     0.69883     0.95996     0.63364     0.70042      0.9553     0.68067     0.64374     0.96314     0.61463     0.64689     0.96711\n",
      "      0.71028     0.74038       0.939     0.58771     0.74951     0.92429     0.67975     0.92366     0.87388     0.63106      0.9301     0.88874 -0.00077981  0.00024447 -8.1599e-05 -0.00041777 -0.00043076 -8.6606e-05  0.00069818 -0.00066221  0.00032367   0.0025791  8.2672e-05  0.00043428  4.9353e-05    0.000126\n",
      "   0.00031364  7.5817e-05  8.5413e-05  0.00037688 -1.6093e-05  0.00022221 -2.5362e-05  9.3699e-05  8.0407e-05  0.00021118  6.0916e-05 -2.5272e-05  6.8486e-05  6.6221e-05  0.00010443  0.00012571  3.7253e-05 -0.00024045  7.4774e-05 -1.4663e-05 -0.00020993  0.00049973  6.1572e-05  5.4359e-05  8.7202e-05 -5.1796e-05\n",
      "  -0.00028765   0.0010604 -7.1883e-05  0.00066841  0.00018549 -9.4175e-06 -0.00026596  0.00039577 -2.9862e-05 -0.00053352  0.00039792 -4.8816e-05 -0.00035167    0.000265 -0.00010985  -0.0013193  0.00042367  -1.049e-05  0.00014204 -0.00090992 -0.00030798  2.1875e-05  -0.0010246  0.00011772]]\n",
      "Shape 1:  (30, 128)\n"
     ]
    }
   ],
   "source": [
    "# Check Windows files (npy files)\n",
    "dados = np.load(f\"{OUTPUT_DIR}/001_aug1_flip_gamma/001_aug1_flip_gamma_010_0.npy\")\n",
    "print(dados[:1])\n",
    "print(\"Shape 1: \", dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6816c-44d7-43d0-bc25-012b9eccbb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
